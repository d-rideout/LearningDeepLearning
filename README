dl.f95 : I started here; uses MPI (18jan022)
hello.cu : learning to use CUDA (18jan022)
nn.cu : Let's write a neural network code using CUDA (18jan022)
nn.h : Let's put symbols into a header file (18jan022)
--------------------------------------------------------------------------------
2^NBITS-2 numbers
for each number:

NBITS => NNeurons layer 1 neurons => layer 2 neuron -> output

On GPU: (eventually) each block gets one data element
   	entire network forward and back done in one block
   	each thread gets one neuron

(could vary architecture or hyperparameters across MPI processes)

<0 ==> composite
>0 ==> prime
truth is +/- 1 if got it wrong

Let's use truth-output as error function.
Slope is +/- 1.
Depends only on sign of output.
Sign is opposite that of output.

For now call the entire kernel for each datapoint.  (Can't distribute
data among blocks until they are independent.)
Yes this is really stupid for now!

I think I can decompose the inner products in each layer across threads,
though their size varies with layer.
Let's just use the larger and waste some threads.
